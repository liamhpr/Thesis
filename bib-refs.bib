@misc{kuhn2023semanticuncertaintylinguisticinvariances,
      title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation}, 
      author={Lorenz Kuhn and Yarin Gal and Sebastian Farquhar},
      year={2023},
      eprint={2302.09664},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.09664}}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}}

@misc{tian2023chatgptultimateprogrammingassistant,
      title={Is ChatGPT the Ultimate Programming Assistant -- How far is it?}, 
      author={Haoye Tian and Weiqi Lu and Tsz On Li and Xunzhu Tang and Shing-Chi Cheung and Jacques Klein and Tegawendé F. Bissyandé},
      year={2023},
      eprint={2304.11938},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2304.11938}}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}}

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}}

@InProceedings{10.1007/978-3-642-76153-9_28,
author="Bridle, John S.",
editor="Souli{\'e}, Fran{\c{c}}oise Fogelman
and H{\'e}rault, Jeanny",
title="Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition",
booktitle="Neurocomputing",
year="1990",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="227--236",
abstract="We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct.",
isbn="978-3-642-76153-9"}

@misc{ippolito2019comparisondiversedecodingmethods,
      title={Comparison of Diverse Decoding Methods from Conditional Language Models}, 
      author={Daphne Ippolito and Reno Kriz and Maria Kustikova and João Sedoc and Chris Callison-Burch},
      year={2019},
      eprint={1906.06362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.06362}}

@misc{shi2024thoroughexaminationdecodingmethods,
      title={A Thorough Examination of Decoding Methods in the Era of LLMs}, 
      author={Chufan Shi and Haoran Yang and Deng Cai and Zhisong Zhang and Yifan Wang and Yujiu Yang and Wai Lam},
      year={2024},
      eprint={2402.06925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06925}}

@misc{holtzman2020curiouscaseneuraltext,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09751}}

@misc{fan2018hierarchicalneuralstorygeneration,
      title={Hierarchical Neural Story Generation}, 
      author={Angela Fan and Mike Lewis and Yann Dauphin},
      year={2018},
      eprint={1805.04833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.04833}}

@misc{guo2017calibrationmodernneuralnetworks,
      title={On Calibration of Modern Neural Networks}, 
      author={Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
      year={2017},
      eprint={1706.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.04599}}

@article{H_llermeier_2021,
   title={Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
   volume={110},
   ISSN={1573-0565},
   url={http://dx.doi.org/10.1007/s10994-021-05946-3},
   DOI={10.1007/s10994-021-05946-3},
   number={3},
   journal={Machine Learning},
   publisher={Springer Science and Business Media LLC},
   author={Hüllermeier, Eyke and Waegeman, Willem},
   year={2021},
   month=mar, pages={457–506}}

@misc{kendall2017uncertaintiesneedbayesiandeep,
      title={What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}, 
      author={Alex Kendall and Yarin Gal},
      year={2017},
      eprint={1703.04977},
      archivePrefix={araXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1703.04977}}

@InProceedings{10.1007/978-3-031-66535-6_17,
    author="L{\"o}hr, Timo
    and Ingrisch, Michael
    and H{\"u}llermeier, Eyke",
    editor="Finkelstein, Joseph
    and Moskovitch, Robert
    and Parimbelli, Enea",
    title="Towards Aleatoric and Epistemic Uncertainty in Medical Image Classification",
    booktitle="Artificial Intelligence in Medicine",
    year="2024",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="145--155",
    abstract="Medical domain applications require a detailed understanding of the decision making process, in particular when data-driven modeling via machine learning is involved, and quantifying uncertainty in the process adds trust and interpretability to predictive models. However, current uncertainty measures in medical imaging are mostly monolithic and do not distinguish between different sources and types of uncertainty. In this paper, we advocate the distinction between so-called aleatoric and epistemic uncertainty in the medical domain and illustrate its potential in clinical decision making for the case of PET/CT image classification.",
    isbn="978-3-031-66535-6"}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}}

@misc{murray2018correctinglengthbiasneural,
    title={Correcting Length Bias in Neural Machine Translation}, 
    author={Kenton Murray and David Chiang},
    year={2018},
    eprint={1808.10006},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/1808.10006}}

@article{https://doi.org/10.1002/j.1538-7305.1948.tb01338.x,
    author = {Shannon, C. E.},
    title = {A Mathematical Theory of Communication},
    journal = {Bell System Technical Journal},
    volume = {27},
    number = {3},
    pages = {379-423},
    doi = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x},
    year = {1948}}

@book{Walley1991-WALSRW,
    author = {Peter Walley},
    editor = {},
    publisher = {Chapman \& Hall},
    title = {Statistical Reasoning with Imprecise Probabilities},
    year = {1991}}

@article{Gneiting01032007,
    author = {Tilmann Gneiting and Adrian E Raftery},
    title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
    journal = {Journal of the American Statistical Association},
    volume = {102},
    number = {477},
    pages = {359--378},
    year = {2007},
    publisher = {Taylor \& Francis},
    doi = {10.1198/016214506000001437},
    URL = {https://doi.org/10.1198/016214506000001437},
    eprint = {https://doi.org/10.1198/016214506000001437}}

@book{Levi1980-LEVTEO-7,
    author = {Isaac Levi},
    editor = {},
    publisher = {MIT Press},
    title = {The Enterprise of Knowledge: An Essay on Knowledge, Credal Probability, and Chance},
    year = {1980}}

@article{COZMAN2000199,
    title = {Credal networks},
    journal = {Artificial Intelligence},
    volume = {120},
    number = {2},
    pages = {199-233},
    year = {2000},
    issn = {0004-3702},
    doi = {https://doi.org/10.1016/S0004-3702(00)00029-1},
    url = {https://www.sciencedirect.com/science/article/pii/S0004370200000291},
    author = {Fabio G. Cozman},
    keywords = {Graphical models of inference, Convex sets of probability measures, Bayesian networks, Lower and upper expectations, Robust Bayesian analysis, Independence relations, Graphical d-separation relations},
    abstract = {This paper presents a complete theory of credal networks, structures that associate convex sets of probability measures with directed acyclic graphs. Credal networks are graphical models for precise/imprecise beliefs. The main contribution of this work is a theory of credal networks that displays as much flexibility and representational power as the theory of standard Bayesian networks. Results in this paper show how to express judgements of irrelevance and independence, and how to compute inferences in credal networks. A credal network admits several extensions—several sets of probability measures comply with the constraints represented by a network. Two types of extensions are investigated. The properties of strong extensions are clarified through a new generalization of d-separation, and exact and approximate inference methods are described for strong extensions. Novel results are presented for natural extensions, and linear fractional programming methods are described for natural extensions. The paper also investigates credal networks that are defined globally through perturbations of a single network.}
}

@misc{löhr2025credalpredictionbasedrelative,
    title={Credal Prediction based on Relative Likelihood}, 
    author={Timo Löhr and Paul Hofman and Felix Mohr and Eyke Hüllermeier},
    year={2025},
    eprint={2505.22332},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/2505.22332}}

@misc{hendrycks2018baselinedetectingmisclassifiedoutofdistribution,
    title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}, 
    author={Dan Hendrycks and Kevin Gimpel},
    year={2018},
    eprint={1610.02136},
    archivePrefix={arXiv},
    primaryClass={cs.NE},
    url={https://arxiv.org/abs/1610.02136}}

@article{10.1162/neco.1992.4.3.448,
    author = {MacKay, David J. C.},
    title = {A Practical Bayesian Framework for Backpropagation Networks},
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {448-472},
    year = {1992},
    month = {05},
    abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.3.448},
    url = {https://doi.org/10.1162/neco.1992.4.3.448},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/3/448/812348/neco.1992.4.3.448.pdf}}

@misc{gal2016dropoutbayesianapproximationrepresenting,
    title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, 
    author={Yarin Gal and Zoubin Ghahramani},
    year={2016},
    eprint={1506.02142},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1506.02142}}

@misc{lakshminarayanan2017simplescalablepredictiveuncertainty,
    title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
    author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
    year={2017},
    eprint={1612.01474},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1612.01474}}

@article{Abellán01022006,
    author = {J. Abellán and G.J. Klir and S. Moral},
    title = {Disaggregated total uncertainty measure for credal sets},
    journal = {International Journal of General Systems},
    volume = {35},
    number = {1},
    pages = {29--44},
    year = {2006},
    publisher = {Taylor \& Francis},
    doi = {10.1080/03081070500473490},
    URL = {https://doi.org/10.1080/03081070500473490},
    eprint = {https://doi.org/10.1080/03081070500473490}}

@article{Birnbaum01061962,
    author = {Allan Birnbaum},
    title = {On the Foundations of Statistical Inference},
    journal = {Journal of the American Statistical Association},
    volume = {57},
    number = {298},
    pages = {269--306},
    year = {1962},
    publisher = {Taylor \& Francis},
    doi = {10.1080/01621459.1962.10480660},
    URL = {https://doi.org/10.1080/01621459.1962.10480660},
    eprint = {https://doi.org/10.1080/01621459.1962.10480660}}

@article{https://doi.org/10.2307/3315449,
    author = {Wasserman, Larry A.},
    title = {Belief functions and statistical inference},
    journal = {Canadian Journal of Statistics},
    volume = {18},
    number = {3},
    pages = {183-196},
    keywords = {Belief functions, Choquet capacities, robust Bayesian inference, posterior bounds, upper and lower probabilities},
    doi = {https://doi.org/10.2307/3315449},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3315449},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.2307/3315449},
    abstract = {Abstract The Dempster Shafer theory of belief functions is a method of quantifying uncertainty that generalizes probability theory. We review the theory of belief functions in the context of statistical inference. We mainly focus on a particular belief function based on the likelihood function and its application to problems with partial prior information. We also consider connections to upper and lower probabilities and Bayesian robustness.},
    year = {1990}}

@article{https://doi.org/10.1111/1467-9868.00205,
    author = {Walley, P. and Moral, S.},
    title = {Upper probabilities based only on the likelihood function},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume = {61},
    number = {4},
    pages = {831-847},
    keywords = {Coherence, Contamination neighbourhood, Finite population, Foundations of statistics, Imprecise probability, Likelihood function, Likelihood inference, Plausibility measure, Profile likelihood, Sure loss, Uniform prior, Upper probability},
    doi = {https://doi.org/10.1111/1467-9868.00205},
    url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00205},
    eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00205},
    abstract = {In the problem of parametric statistical inference with a finite parameter space, we propose some simple rules for defining posterior upper and lower probabilities directly from the observed likelihood function, without using any prior information. The rules satisfy the likelihood principle and a basic consistency principle (‘avoiding sure loss’), they produce vacuous inferences when the likelihood function is constant, and they have other symmetry, monotonicity and continuity properties. One of the rules also satisfies fundamental frequentist principles. The rules can be used to eliminate nuisance parameters, and to interpret the likelihood function and to use it in making decisions. To compare the rules, they are applied to the problem of sampling from a finite population. Our results indicate that there are objective statistical methods which can reconcile three general approaches to statistical inference: likelihood inference, coherent inference and frequentist inference.},
    year = {1999}}

@incollection{epub31329,
    year = {2012},
    editor = {Salvatore Greco and Bernadette Bouchon-Meunier and Giulianella Coletti and Mario Fedrizzi and Benedetto Matarazzo and Ronald R. Yager},
    pages = {491--500},
    title = {Likelihood-Based Robust Classification with Bayesian Networks},
    address = {Berlin},
    booktitle = {Advances in Computational Intelligence. 14th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, IPMU 2012, Catania, Italy, July 9-13, 2012, Proceedings, Part III},
    author = {Alessandro Antonucci and Marco E. G. V. Cattaneo and Giorgio Corani},
    publisher = {Springer},
     url = {}}

@misc{malinin2021uncertaintyestimationautoregressivestructured,
    title={Uncertainty Estimation in Autoregressive Structured Prediction}, 
    author={Andrey Malinin and Mark Gales},
    year={2021},
    eprint={2002.07650},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/2002.07650}}

@misc{lu2024mergeensemblecooperatesurvey,
    title={Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models}, 
    author={Jinliang Lu and Ziliang Pang and Min Xiao and Yaochen Zhu and Rui Xia and Jiajun Zhang},
    year={2024},
    eprint={2407.06089},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2407.06089}}

@inproceedings{chen2025abgsciqa,
    title={Abg-Sci{QA}: A dataset for Understanding and Resolving Ambiguity in Scientific Questions},
    author={Tiejin Chen and Kuan-Ru Liou and Mithun Shivakoti and Aaryan Gaur and Pragya Kumari and Meiqi Guo and Hua Wei},
    booktitle={ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models},
    year={2025},
    url={https://openreview.net/forum?id=GvI3QvhIum}}

@inproceedings{guo2021abgcoqa,
    title={Abg-Co{QA}: Clarifying Ambiguity in Conversational Question Answering},
    author={Meiqi Guo and Mingda Zhang and Siva Reddy and Malihe Alikhani},
    booktitle={3rd Conference on Automated Knowledge Base Construction},
    year={2021},
    url={https://openreview.net/forum?id=SlDZ1o8FsJU}}

@misc{liu2025uncertaintyquantificationconfidencecalibration,
    title={Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey}, 
    author={Xiaoou Liu and Tiejin Chen and Longchao Da and Chacha Chen and Zhen Lin and Hua Wei},
    year={2025},
    eprint={2503.15850},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2503.15850}}

@misc{he2021debertadecodingenhancedbertdisentangled,
    title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention}, 
    author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
    year={2021},
    eprint={2006.03654},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2006.03654}}

@misc{kadavath2022languagemodelsmostlyknow,
    title={Language Models (Mostly) Know What They Know}, 
    author={Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zac Hatfield-Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
    year={2022},
    eprint={2207.05221},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2207.05221}}

@misc{fomicheva2020unsupervisedqualityestimationneural,
    title={Unsupervised Quality Estimation for Neural Machine Translation}, 
    author={Marina Fomicheva and Shuo Sun and Lisa Yankovskaya and Frédéric Blain and Francisco Guzmán and Mark Fishel and Nikolaos Aletras and Vishrav Chaudhary and Lucia Specia},
    year={2020},
    eprint={2005.10608},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2005.10608}}
