% !TEX root = ../main.tex
%
\chapter{Related Work}
\label{sec:related_work}
The reliable quantification of uncertainty in LLMs has garnered 
significant attention in recent years. As researchers seek to mitigate
hallucinations and improve model safety, various metrics have been 
proposed to estimate when a model's generation is likely to be 
incorrect. This chapter reviews the evolution of these methods,   
starting from traditional uncertainty quantification (UQ) metrics, and 
culminating in state-of-the-art approaches like Semantic Entropy,
which serves as the primary baseline for the credal prediction
methodology developed in this thesis.

\section{Traditional Uncertainty Quantification (UQ)}
\label{sec:related_work:traditional}
Applying traditional UQ techniques 
to large language models presents substantial challenges.
The computational overhead of Deep Ensembles 
\parencite{lakshminarayanan2017simplescalablepredictiveuncertainty}
or Bayesian frameworks like Monte Carlo dropout 
\parencite{gal2016dropoutbayesianapproximationrepresenting}
makes them impractical for natural language generation (NLG) in 
massive models. There are also sequence-level metrics like 
sequence-level predictive entropy, which estimates the uncertainty 
across all possbile output sequences
\parencite{malinin2021uncertaintyestimationautoregressivestructured}.
While sequence-level metrics represent an improvement, they suffer
from a fundamental flaw in the context of open-ended free-form 
question answering: they are strictly tied to the exact lexical
surface form of the text. Because human language is highly flexible,
an LLM might distribute its probability mass across dozens of 
differently phrased sequnces that all share the exact same meaning,
e.g. "The capital of France is Paris" vs. "Paris is the French 
capital".
Traditional sequence-level metrics treat these lexical variations as 
disjoint, competing hypotheses. Consequently, if the model predicts
many synonymous answers, standard sequence entropy will register a 
highly flat distribution and artificially inflate the uncertainty
score.

This critical limitation necessitated a paradigm shift from lexical
(word-based) probability to semantic (meaning-based) probability, 
leading to the development of Semantic Entropy 
\parencite{kuhn2023semanticuncertaintylinguisticinvariances}.


\section{Semantic Entropy}
\label{sec:related_work:semantic_entropy}
To resolve the conflation of lexical flexibility and factual ignorance,
\textcite{kuhn2023semanticuncertaintylinguisticinvariances} 
introduced the concept of \textbf{Semantic Entropy (SE)}.
Semantic Entropy shifts the unit of uncertainty measurement from the  
exact sequence of words to the underlying meaning, or 
\textit{semantics}, of the generated text.
Because SE serves as both the primary baseline and a core mechanical
component of the methodology proposed in this thesis, we detail its 
formal computation below. The SE itself is computed in three steps:
\begin{enumerate}
    \item Generating $M$ sequences by the same model
    \item Clustering the generated sequences by semantic equivalence
    \item Computation of the semantic entropy
\end{enumerate}


\subsection{Semantic Clustering}
\label{sec:related_work:semantic_entropy:clustering}
The core premise of SE is that the infinite space of possible lexical 
sequences generated by an LLM can be partitioned  into a finite set of 
discrete meanings.

Let $x$ be the input prompt, and let $\mathcal{S} = 
\{s^{(1)}, s^{(2)}, \dots, s^{(M)}\}$ be a set of $M$ sequences 
generated by the model. We define a bidirectional semantic 
equivalence relation between sequences, denoted as 
$E(\cdot, \cdot)$, which holds true if sequence
$s^{(i)}$ and sequence $s^{(j)}$ convey the exact same factual 
information, i.e., they entail, in the context of the 
\hbox{prompt $x$.}
This equivalence relation groups the generated sequences into distinct
\textbf{semantic clusters} (or equivalence classes), denoted as 
$C = \{c_1, c_2, \dots, c_K\}$, where 
\hbox{$\forall s^{(i)}, s^{(j)} \in c_k: E(s^{(i)}, s^{(j)})$} and 
$K \leq M$. 
In practice, this clustering is performed using an automated 
evaluation model, such as a Natural Language Inference (NLI) classifier
(\textcite{kuhn2023semanticuncertaintylinguisticinvariances} 
used the DeBERTa-large-MNLI model
\parencite{he2021debertadecodingenhancedbertdisentangled}).


\subsection{Computing the Semantic Entropy}
\label{sec:related_work:semantic_entropy:computing}
Once the sequences are clustered, the probability of a specific meaning
(a cluster $c_k \in C$) is computetd by marginalizing over the 
probabilities of all individual lexical sequences within that cluster.
The cluster probability is given by:
\begin{equation}
    P(c_k \mid x) = \sum_{s \in c_k} P(s \mid x)
    \label{eq:cluster_probability}
\end{equation}

By treating the clusters as the new categorical variables,
the SE is calculated as the Shannon Entropy over the cluster 
probabilities:

\begin{equation}
    \text{SE}(x) = - \sum_{k=1}^{K} P(c_k \mid x) \log P(c_k \mid x)
    \label{eq:semantic_entropy}
\end{equation}

Because we can not know every possible meaning-class $c$, we can only 
sample $c$ from the given distribution by the model. Thus  
\textcite{kuhn2023semanticuncertaintylinguisticinvariances}
use Monte Carlo integration over the semantic equivalence classes 
to estimate the expectation in equation \ref{eq:semantic_entropy}:

\begin{equation}
    \text{SE}(x) \approx -|C|^{-1} \sum_{i=1}^{|C|}\log P(C_i \mid x)
    \label{eq:semantic_entropy_estimation}
\end{equation}

where

\begin{equation}
    P(C_i \mid x) = \frac{P(c_i \mid x)}{\sum_{c \in C} P(c \mid x)}
    \label{eq:nn_cluster_likelihood}
\end{equation}
