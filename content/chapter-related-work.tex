% !TEX root = ../main.tex
%
\chapter{Related Work}
\label{sec:related_work}
The reliable quantification of uncertainty in LLMs has garnered 
significant attention in recent years. As researchers seek to mitigate
hallucinations and improve model safety, various metrics have been 
proposed to estimate when a model's generation is likely to be 
incorrect. This chapter reviews the evolution of these methods,   
starting from traditional uncertainty quantification (UQ) metrics, and 
culminating in state-of-the-art approaches like Semantic Entropy,
which serves as the primary baseline for the credal prediction
methodology developed in this thesis.

\section{Traditional Uncertainty Quantification (UQ)}
\label{sec:related_work:traditional}
Applying traditional UQ techniques 
to LLMs presents substantial challenges.
The computational overhead of Deep Ensembles 
\parencite{lakshminarayanan2017simplescalablepredictiveuncertainty}
or Bayesian frameworks like Monte Carlo dropout 
\parencite{gal2016dropoutbayesianapproximationrepresenting}
makes them impractical for natural language generation (NLG) in 
massive models. 
Initial approaches to estimating LLM uncertainty directly adapted
standard classificatoin metrics or relied on heuristic comparisons.
These traditional baselines generally fall into three categories: 
predictive entorpy, lexical similarity and self-evaluation.

\textbf{Predictive Entropy Methods} \\
The most straightforward methods evaluate the model's confidence 
using the probability distributions generated during decoding. 
Token-level entropy evaluates local uncertainty but fails to capture
the holistic confidence of a factual claim. To address this,  
\textcite{malinin2021uncertaintyestimationautoregressivestructured}
proposed sequence-level predictive entorpy (often implemented as 
length-normalized entropy), which estimates uncertainty across all
possible output sequences. However, because these methods operate
strictly on exact token matches, they artificially inflate uncertainty
when an LLM distributes probability mass across multiple valid 
phrasings of the same answer.

\textbf{Lexical Similarity} \\
To account for the fact that generated answers might differ slightly 
in their suface forms, researchsers adapted translation metrics to 
quantify uncertainty. By sampling multiple answers for a given 
question, one can compute the average lexical similarity between 
them using metrics like ROUGE-L or BLEU 
\parencite{fomicheva2020unsupervisedqualityestimationneural}. 
If the sampled answers have high n-gram overlap, the model is deemed 
certain. While an improvement over strict sequence likelihood, 
lexical similarity still struggles with open-ended generation.
An LLM might generate "Berlin" and "The capital of Germany", which 
share zero lexical overlap (low ROUGE-L) but convey the exact same 
meaning, leading to false uncertainty signals.

\textbf{Self-Evaluation} \\
Another distinct approach treats the LLM as an independent evaluator of
its own uncertainty. Methods like $p(\text{True})$ prompt the model 
to evaluate its own generated answer or ask it directly if a proposed 
fact is correct \parencite{kadavath2022languagemodelsmostlyknow}.
While intuitive, self-evaluation baselines are highly sensitive
to the specific phrasing of the prompt. 

All three of these traditional baseline families fail to reliably 
distinguish between benign linguistic flexibility and genuine factual 
ignorance. 
Traditional metrics treat these lexical variations as 
disjoint, competing hypotheses. 
This critical limitation necessitated a paradigm shift from lexical
(word-based) probability to semantic (meaning-based) probability, 
leading to the development of Semantic Entropy 
\parencite{kuhn2023semanticuncertaintylinguisticinvariances}.


\section{Semantic Entropy}
\label{sec:related_work:semantic_entropy}
To resolve the conflation of lexical flexibility and factual ignorance,
\textcite{kuhn2023semanticuncertaintylinguisticinvariances} 
introduced the concept of \textbf{Semantic Entropy (SE)}.
Semantic Entropy shifts the unit of uncertainty measurement from the  
exact sequence of words to the underlying meaning, or 
\textit{semantics}, of the generated text.
Because SE serves as both the primary baseline and a core mechanical
component of the methodology proposed in this thesis, we detail its 
formal computation below. The SE itself is computed in three steps:
\begin{enumerate}
    \item Generating $M$ sequences by the same model
    \item Clustering the generated sequences by semantic equivalence
    \item Computation of the semantic entropy
\end{enumerate}


\subsection{Semantic Clustering}
\label{sec:related_work:semantic_entropy:clustering}
The core premise of SE is that the infinite space of possible lexical 
sequences generated by an LLM can be partitioned  into a finite set of 
discrete meanings.

Let $x$ be the input prompt, and let $\mathcal{S} = 
\{s^{(1)}, s^{(2)}, \dots, s^{(M)}\}$ be a set of $M$ sequences 
generated by the model. We define a bidirectional semantic 
equivalence relation between sequences, denoted as 
$E(\cdot, \cdot)$, which holds true if sequence
$s^{(i)}$ and sequence $s^{(j)}$ convey the exact same factual 
information, i.e., they entail, in the context of the 
\hbox{prompt $x$.}
This equivalence relation groups the generated sequences into distinct
\textbf{semantic clusters} (or equivalence classes), denoted as 
$C = \{c_1, c_2, \dots, c_K\}$, where 
\hbox{$\forall s^{(i)}, s^{(j)} \in c_k: E(s^{(i)}, s^{(j)})$} and 
$K \leq M$. 
In practice, this clustering is performed using an automated 
evaluation model, such as a Natural Language Inference (NLI) classifier
(\textcite{kuhn2023semanticuncertaintylinguisticinvariances} 
used the DeBERTa-large-MNLI model
\parencite{he2021debertadecodingenhancedbertdisentangled}).


\subsection{Computing the Semantic Entropy}
\label{sec:related_work:semantic_entropy:computing}
Once the sequences are clustered, the probability of a specific meaning
(a cluster $c_k \in C$) is computetd by marginalizing over the 
probabilities of all individual lexical sequences within that cluster.
The cluster probability is given by:
\begin{equation}
    P(c_k \mid x) = \sum_{s \in c_k} P(s \mid x)
    \label{eq:cluster_probability}
\end{equation}

\begin{figure}
    \includegraphics[width=\textwidth]{drawio/SemanticEntropy.png}
    \caption{Visualization of Semantic Clustering}
    \label{fig:related_work:semantic_entropy:semantic_clustering}
\end{figure}

By treating the clusters as the new categorical variables,
the SE is calculated as the Shannon Entropy over the cluster 
probabilities:

\begin{equation}
    \text{SE}(x) = - \sum_{k=1}^{K} P(c_k \mid x) \log P(c_k \mid x)
    \label{eq:semantic_entropy}
\end{equation}

Because we can not know every possible meaning-class $c$, we can only 
sample $c$ from the given distribution by the model. Thus  
\textcite{kuhn2023semanticuncertaintylinguisticinvariances}
use Monte Carlo integration over the semantic equivalence classes 
to estimate the expectation in equation \ref{eq:semantic_entropy}:

\begin{equation}
    \text{SE}(x) \approx -|C|^{-1} \sum_{i=1}^{|C|}\log P(C_i \mid x)
    \label{eq:semantic_entropy_estimation}
\end{equation}

where

\begin{equation}
    P(C_i \mid x) = \frac{P(c_i \mid x)}{\sum_{c \in C} P(c \mid x)}
    \label{eq:nn_cluster_likelihood}
\end{equation}

SE represents a significant advancement over raw 
sequence likelihood. But it relies on a single, static probability
distribution. In the standard implementation by 
\textcite{kuhn2023semanticuncertaintylinguisticinvariances}, 
sequences are sampled, and probabilities are evaluated at a single, 
fixed temperature 
(\textcite{kuhn2023semanticuncertaintylinguisticinvariances} 
state that $\tau=0.5$ is optimal). 

As established in Section \ref{sec:background:credal_sets}, precise
probabilities are brittle when a model is miscalibrated or highly 
ignorant. This gap motivates the primary contribution of this thesis.
Instead of computing SE on a single distribution, we propose computing
it across a \textbf{Credal Set of distributions} generated by a valid
range of temperatures. By analyzing how semantic cluster probabilities
fluctate across this set, yielding upper and lower cluster bounds, we
detect epistemic uncertainty that standard SE is blind to.


\section{Credal Prediction based on Relative Likelihood}
\label{sec:related_work:credal_prediction}
While SE addresses the issue of lexical equivalence in LLMs, it lacks
a mechanism to handle severe epistemic uncertainty arising from a
poorly calibrated, single probability distribution. To address this,
we look to recent advancements in the field of imprecise probabilities,
specifically the application of Credal Sets to deep learning.

\subsection{Relative Likelihood Ensembles}
\label{sec:related_work:credal_prediction:rel_ll_ensembles}
Recently, \textcite{l√∂hr2025credalpredictionbasedrelative}
proposed a theoretically grounded approach to credal prediction
based on the statistical notion of relative likelihood. Instead
of relying on a single maximum likelihood estimator (MLE),
their framework defines the prediction target as a credal set induced 
by all plausible models whose relative likelihood exceeds a specific 
threshold $\alpha$ (the $\alpha$-cut). Specifically, they introduce the
Credal Relative Likelihood ($CreRL_\alpha$) method, which trains an 
ensemble of neural networks using an early-stopping strategy. Each 
model in the ensemble is trained until it reaches a sprecific relative 
likelihood threshold $\tau_i$, ensuring that the final ensemble spans
the entire valid $\alpha$-cut.
To guarantee diversity among the ensemble members, they further employ
a novel "ToBias" initialization strategy, which initializes the 
models to predict degenerate distributions at the vertices of the 
probability simplex. 


\subsection{Bridging the Gap: From Ensembles to LLM Temperatures}
\label{sec:related_work:credal_prediction:bridge}
The $CreRL_\alpha$ framework successfully demonstrates that relative
likelihood can be used to construct robust credal sets, achieving
superior coverage on standard classifiction benchmarks. However, this
approach, like other credal deep learning models (e.g., Credal 
Bayesian Neural Networks), is designed for standard discriminative 
classification tasks where the model weights are optimized during 
training. 

Applying this weight-space ensemble directly to modern LLMs is 
computationally prohibitive due to their massive size, and it does
not cleanly translate to the autoregressive, open-ended generation
of sequences.
